---
title: "compare_errors_and_summarizers"
output: html_document
---

```{r}
library(CopulaSEJ)
source("dev_utils.R")

file_name <- "output/data49_nov24.rds"
studies <- readRDS(file_name)
studies <- filter_studies_few_questions(studies, min_questions=11)
studies <- filter_study_remove_ids(studies, study_ids=7) 
list_mod <- change_value_in_study_list(studies, k_percentiles_to_colname(c(5, 50, 95)), 0, 0.0001)$study_list
```


```{r}
data_list_short <- list_mod[1]
error_metrics = list(cdf_independence_cond_m, get_ratio_error_metric(), get_linear_error_metric())
summarizing_functions = list(get_three_quantiles_summarizing_function(), get_median_summarizing_function())
results <- list()
warnings <- list()
for (error_metric in error_metrics) {
  for (summarizing_function in summarizing_functions) {
    print(paste("Analyzing error metric:", error_metric$name, ", and summarizing function:", summarizing_function$name))
    
    params <- default_simulation_params(copula_model = "vine", prediction_method = "copula", error_metric = error_metric, summarizing_function = summarizing_function)
    
    analysis_res <- run_analysis_per_study(data_list_short, params)
    analysis_res$results["prediction_method"] <- glue::glue("copula:{error_metric$name}:{summarizing_function$name} summarizer")
    results <- append(results, list(analysis_res$results))
    warnings <- append(warnings, list(analysis_res$warnings))
  }
}
results_df <- dplyr::bind_rows(results)


```


```{r}
# create a function that creates a prediction from a posterior function

preds <- results_df$posterior |> purrr::map_dbl(\(post) mean(sample_log_unnormalized_density(post$logDM, post$support, 1000)), .progress = TRUE)
```


