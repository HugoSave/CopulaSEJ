---
title: "copula_performance"
output: html_document
---

```{r setup}
library(tidyverse)
library(ggplot2)
library(purrr)
library(knitr)
library(patchwork)
library(copula)
library(scales)
library(knitr)
source("dev_utils.R")

#file_name <- "output/data49_nov24.rds"
studies <- load_data_49(relative_dev_folder = TRUE)
studies <- load_data_47(relative_dev_folder = TRUE)
# id_high_mag <- find_high_magnitude_differences(studies, max_mag_ratio = 1000)
#id_high_mag  <- id_high_mag |> rename(test_question=question_id)
#
#percentile_names <- k_percentiles_to_colname(c(5, 50, 95))
#
#list_mod <- change_value_in_study_list(studies, percentile_names, 0, 0.0001)
#studies_no_m_zero <- list_mod[[1]]
#modified_studies <- list_mod[[2]]

```

```{r}

#saveRDS(results_df, "dev/output/performance_comparison_new.rds")
#results_df<-readRDS("output/performance_comparison_new.rds")
copula_method_df <- readRDS("output/compare_performance_simulation.rds")$results
#copula_method_df <- copula_method_df |> filter(prediction_method %in% c("CohierarchicalCDF3QNoRCT(0.7)", "CoInCDF3QNoRNoCT"))
benchmark_df <- readRDS("output/benchmarking_methods_performance.rds")$results
benchmark_df <- benchmark_df |> filter(study_id !=7) # remove study_id 7, not in the normal performance 
benchmark_df <- benchmark_df |> filter(prediction_method != "MA") 
benchmark_df <- benchmark_df |> filter(study_id !=48) # remove study_id 7, not in the normal performance 

stopifnot(nrow(copula_method_df)/length(unique(copula_method_df$prediction_method)) == nrow(benchmark_df)/length(unique(benchmark_df$prediction_method)))

results_df <- rbind(copula_method_df, benchmark_df)
nr_prediction_methods = length(unique(results_df$prediction_method))
nr_tests = nrow(results_df)/nr_prediction_methods
print(paste("Number of prediction methods: ", nr_prediction_methods))
print(paste("Number of tests: ", nr_tests))
#results_df <- readRDS("output/performance_comparison.rds")

results_df_small <- results_df |> filter(realization <= 1000)
results_df_small <- results_df |> anti_join(id_high_mag, by=c("study_id", "test_question"))
results_df |> filter(is.finite(rel_error)) |> filter(rel_error==max(rel_error))
#results_df <- results_df |> mutate(prediction_method = if_else(str_starts(prediction_method, "DP"), "product", prediction_method))

# results_df <- get_simulation_group_files(rel_dev=TRUE) |> load_files() |> combine_simulation_results()
# results_df <- results_df$results

pos_values <- results_df |> filter(realization>0 & prediction>0) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = "Postivie Predicted and Realized Values",
       x = "Realized Value",
       y = "Predicted Value") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal()

abs_values <- results_df |> filter(realization!=0 & prediction!=0) |> mutate(realization=abs(realization), prediction=abs(prediction)) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = "Predicted and Realized Values",
       x = "abs(Realized Value)",
       y = "abs(Predicted Value)") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal()

pos_values

abs_values
```
```{r}
get_uniformity_metrics_per_method <- function(df) {
  df |> split(df$prediction_method) |>
    purrr::map(\(df) {
      metrics <- calculate_uniformity_metrics(df$cum_prob)
      tibble::tibble_row(
        prediction_method = unique(df$prediction_method),
        !!!metrics
      )
    })  |> purrr::list_rbind()
}
# calculate the 

get_error_metrics_per_method <- function(df) {
  df |> group_by(prediction_method) |> 
    summarise(
      MAE_median = mean(abs(median-realization)),
      MedAE_median = median(abs(median-realization)),
      RMSE_mean = sqrt(mean((mean-realization)^2)),
      MedSE_mean = median((mean-realization)^2),
      MedAPE = median(abs(median-realization)/realization),
      MedAPE = median(abs((median-realization)/realization)),
      .groups = "drop"
    )
}

get_all_point_metrics_per_method <- function(df) {
  get_uniformity_metrics_per_method(df) |> 
    full_join(get_error_metrics_per_method(df), by="prediction_method")
}

copula_method_metrics <- get_all_point_metrics_per_method(copula_method_df)

copula_method_metrics |> arrange(L1)

add_columns_from_prediction_method <- function(df) {
  df |> mutate(
    decoupler = case_when(
      str_detect(prediction_method, "CDF") ~ "CDF",
      str_detect(prediction_method, "Rel") ~ "Rel",
      .default = prediction_method
    ),
    copula = case_when(
      str_detect(prediction_method, "In") ~ "Indep",
      str_detect(prediction_method, "Hi") ~ "Gaussian",
      TRUE ~ NA_character_
    ),
    marginal_method = case_when(
      str_detect(prediction_method, "PE") ~ "PE",
      str_detect(prediction_method, "MAP") ~ "NoCT",
      str_detect(prediction_method, "3Q") ~ "3Q",
      str_detect(prediction_method, "EW") ~ "EW",
      str_detect(prediction_method, "DP") ~ "DP",
      TRUE ~ NA_character_
    )
  )
}

copula_method_metrics <- add_columns_from_prediction_method(copula_method_metrics) 

add_parseable_metric_names <- function(df) {
  df |> mutate(
  `L['Unif']^1` = L1,
  `MedAE['Median']` = MedAE_median,
  `MedSE['Mean']` = MedSE_mean
)
}

copula_method_metrics <- add_parseable_metric_names(copula_method_metrics)
# important metrics
focus_metrics <- c("L['Unif']^1", "MedAE['Median']", "MedSE['Mean']")
# select these metrics and pivot wider


create_per_method_metrics_plot <- function(df) {
  metrics_long <- df |> 
  select(prediction_method, copula, decoupler, all_of(focus_metrics)) |> 
  pivot_longer(-c(prediction_method, decoupler, copula), names_to = "metric", values_to = "value") |> 
  mutate(metric = factor(metric, levels = focus_metrics))
  metrics_long |> ggplot(aes(prediction_method, value, color=decoupler, shape=copula)) +
    geom_point() +
    labs(title = NULL,
         x = "Prediction Method",
         y = "Metric Value") +
    facet_grid(rows = vars(metric), scales="free_y", labeller=label_parsed) + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

copula_methods_point_metric_p <- copula_method_metrics |> 
  create_per_method_metrics_plot()

ggsave("output/copula_methods_point_metric.pdf", copula_methods_point_metric_p, width = 10, height = 10)
copula_methods_point_metric_p
```
```{r}
# extract a subset of copula prediction modls of interest
copula_method_df$prediction_method |> unique() |> sort()

methods_of_interest <- c("His(Rel.)BetaMAP(0.5)CT(0.7)prior(0.5)", "Ins(Rel.)BetaMAP(0.5)CT(0.7)prior(0.5)")

copula_method_df_subset <- copula_method_df |> filter(prediction_method %in% methods_of_interest)

results_df <- rbind(benchmark_df, copula_method_df_subset)
```


```{r, fig.width=10}

results_df_small |> filter(realization!=0 & prediction!=0) |> mutate(realization=abs(realization), prediction=abs(prediction)) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = "Predicted and Realized Values",
       x = "abs(Realized Value)",
       y = "abs(Predicted Value)") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal() + # remove legend 
  theme(legend.position = "none")

abs_values <- results_df |> filter(realization!=0 & prediction!=0) |> mutate(realization=abs(realization), prediction=abs(median)) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = NULL,
       x = "abs(Realized Value)",
       y = "abs(Median)") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal() + # remove legend 
  theme(legend.position = "none")

ggsave("output/abs_values.pdf", abs_values, width = 10, height = 14)
abs_values
```
```{r}

results_metric_table <- get_all_point_metrics_per_method(results_df) |>
  add_parseable_metric_names() |>
  add_columns_from_prediction_method()

create_per_method_metrics_plot(results_metric_table)
```

# Look at relative error
```{r}
# count number of infinite rel_error
nr_0_realizations <- results_df |> filter(rel_error == Inf) |> nrow() / nr_prediction_methods
# should yield the same
nr_0_realizations_other <- results_df |> filter(realization == 0)  |> nrow() / nr_prediction_methods
stopifnot(nr_0_realizations == nr_0_realizations_other)
print(paste("Number of infinite relative errors: ", nr_0_realizations))


relative_erros_boxplot <- results_df |> filter(realization != 0 & abs(rel_error) < 10 & study_id <= 20) |> # filter(!(prediction_method %in% c("copula"))) |>
  ggplot(aes(prediction_method, rel_error)) +
  geom_boxplot() +
  labs(x = "method",
       y = "Relative Error") +
  theme_minimal() +
  ylim(-10, 10) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("output/relative_erros_boxplot.png", relative_erros_boxplot, width=10)
relative_erros_boxplot 
```

```{r}
# plot the emperical ecdf of the cum_prob  
ecdfs_subset <- results_df |> filter(cum_prob != 1 & cum_prob != 0) |> ggplot(aes(x = cum_prob)) +
  stat_ecdf(geom = "step") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  facet_wrap(~ prediction_method) +
  labs(
    title = NULL,
    x = "Cumulative Probability",
    y = "Empirical CDF"
  ) +
  theme_bw()
ggsave("output/ecdfs_subset.pdf", ecdfs_subset, width = 10, height = 6)
ecdfs_subset
```
```{r}
results_df <- results_df |> mutate(
  MAE_median = mean(abs(median-realization)),
  RMSE_mean = sqrt(mean((mean-realization)^2))
)
```

```{r}
methods_with_densities <- results_df |> filter(!is.na(cum_prob))

# calculate the 
uniformity_metrics <- methods_with_densities |> split(methods_with_densities$prediction_method) |>
  purrr::map(\(df) {
    metrics <- calculate_uniformity_metrics(df$cum_prob)
    tibble::tibble_row(
      prediction_method = unique(df$prediction_method),
      !!!metrics
    )
  })  |> purrr::list_rbind()

error_metrics <- results_df |> group_by(prediction_method) |> 
  summarise(
    MAE_median = mean(abs(median-realization)),
    MedAE_median = median(abs(median-realization)),
    RMSE_mean = sqrt(mean((mean-realization)^2)),
    MedAPE = median(abs(median-realization)/realization),
    .groups = "drop"
  )
point_metrics <- uniformity_metrics |> left_join(error_metrics, by="prediction_method")

point_metrics |> arrange(L1) 
```


```{r}
# some plot that per study shows the prediction of a specific method

max_rel_error_in_plt = 5

nr_studies <- max(results_df$study_id)
breaks <- unique(c(1, seq(from=10, to=nr_studies, by=10), nr_studies))
minor_b <- 1:nr_studies

rel_error_per_plot <- results_df |> filter(realization!=0 & abs(rel_error) < max_rel_error_in_plt) |> ggplot(aes(study_id, rel_error, group=study_id)) +
  geom_boxplot() + facet_wrap(facets=vars(prediction_method), ncol=1) + 
  scale_x_continuous(breaks = breaks, minor_breaks=minor_b, limits= c(1, nr_studies)) +
  labs(x = "Study ID",
       y = "Relative Error")

nr_filtered_points <- results_df |> filter(realization!=0 & abs(rel_error) < max_rel_error_in_plt) |> nrow()
nr_away_filtered_points <- results_df |> nrow() - nr_filtered_points
# lowest number of points for a study

ggsave("output/rel_error_per_plot_generalization.pdf", rel_error_per_plot, width = 8, height = 10)
rel_error_per_plot
```
```{r}
neg_log_lik_per_study <- ggplot(results_df |> filter(is.finite(neg_log_lik)), aes(study_id, neg_log_lik, group=study_id)) +
  geom_boxplot() + facet_grid(rows=vars(prediction_method)) + 
  scale_x_continuous(breaks = breaks, minor_breaks=minor_b) +
  labs(x = "Study ID",
       y = "Negative Log Likelihood") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
neg_log_lik_per_study
```

### Point results all samples
```{r}
options(pillar.print_min = 11)
results_df |> filter(realization!=0 & !is.na(rel_error)) |> group_by(prediction_method) |> summarise(MAPE = mean(abs(rel_error)),MedAPE = median(abs(rel_error)), RMSPE = sqrt(mean(rel_error^2)), MAXPE = max(abs(rel_error)), MedNLL=median(neg_log_lik)) 

```
### Point results rel_error less than 10
```{r}
results_df |> filter(realization!=0 & abs(rel_error) < 100 &  !is.na(rel_error)) |> group_by(prediction_method) |>  summarise(MAPE = mean(abs(rel_error)),MedAPE = median(abs(rel_error)), RMSPE = sqrt(mean(rel_error^2)), MAXPE = max(abs(rel_error)), MedNLL=median(neg_log_lik))
```
```{r}
results_df |> mutate(MAPE=abs(rel_error)) |> filter(realization!=0 & abs(rel_error) < 10 & !is.na(MAPE)) |> group_by(prediction_method) |>  summarise(MAPE = mean(MAPE),MedAPE = median(MAPE), RMSPE = sqrt(mean(MAPE^2)), MAXPE = max(MAPE), MedNLL=median(neg_log_lik))
```


```{r}
best_prediction_method_per_test <- results_df  |> group_by(study_id, test_question) |> 
  summarise(best_rel_error = min(abs(rel_error)), .groups="drop")

# join best_prediction_method_per_test and results_df
result_with_best <- results_df |> left_join(best_prediction_method_per_test, by=c("study_id", "test_question"))

times_equal_to_best_rel_error <- result_with_best |> rowwise() |>
  mutate(equal_to_best_rel_error = if_else(abs(rel_error) == best_rel_error, T, F)) |>
  group_by(prediction_method) |> summarise(n_equal_to_best_rel_error = sum(equal_to_best_rel_error), 
                                           ratio_best=n_equal_to_best_rel_error/nr_tests, .groups="drop") 
  
```
```{r}
pair_performance_comparision <- function(df, method1, method2) {
  df |> filter(realization!=0 & (prediction_method %in% c(method1, method2))) |> 
    group_by(study_id, test_question) |> 
    summarise(abs_rel_error_1 = abs(rel_error[prediction_method==method1]), 
              abs_rel_error_2 = abs(rel_error[prediction_method==method2]), .groups="drop") |> 
    mutate(diff = abs_rel_error_1 - abs_rel_error_2) |> 
    filter(diff!=0) |>
    summarise(method_one_better=sum(diff<0), 
              ratio=method_one_better/n(),
              #mean_diff = mean(diff), 
              #sd_diff = sd(diff), 
              n=length(diff))
}
method1 = "CoInCDF3QNoRNoCT"
method2 = "EW"
table_pair <- pair_performance_comparision(results_df, method1, method2)
table_pair |> rename("Hiearchry better than EW"="method_one_better")
```
```{r}
sorted_ew <- results_df |> filter(realization!=0 & (prediction_method %in% c(method1, method2))) |> 
    group_by(study_id, test_question) |> 
    summarise(abs_rel_error_1 = abs(rel_error[prediction_method==method1]), 
              abs_rel_error_2 = abs(rel_error[prediction_method==method2]), .groups="drop") |>
  filter(abs_rel_error_1 < 10 & abs_rel_error_2 < 10) 


N = nrow(sorted_ew)
sorted_ew |> ggplot(aes(x=1:N,y=abs_rel_error_1)) + geom_line() +
  geom_line(aes(x=1:N,y=abs_rel_error_2), color="red") +
  labs(title = "Absolute Relative Error",
       x = "Study ID",
       y = "Absolute Relative Error") +
  scale_x_continuous(breaks = seq(1, N, by=10)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# sort by rel error 1
sorted_ew <- arrange(sorted_ew, abs_rel_error_1)
sorted_rel1 <- sorted_ew$abs_rel_error_1
sorted_rel2 <- arrange(sorted_ew, abs_rel_error_2)$abs_rel_error_2
# plot both sortelrel1 and 2 in same plot
plot(1:N, sorted_rel1, sorted_rel2)

# pivot longer
sorted_ew <- sorted_ew |> pivot_longer(-c(study_id, test_question), names_to="prediction_method", values_to = "abs_rel_error") 

pair_selection <- results_df |> filter(realization!=0 & (prediction_method %in% c(method1, method2))) |> select(study_id, test_question, prediction_method, rel_error) 
pair_selection$id <- interaction(pair_selection$study_id, pair_selection$test_question)
pair_selection |> filter(abs(rel_error) < 10) |> ggplot(aes(x=id, y=abs(rel_error), color=prediction_method)) + geom_point()
```



```{r}
nr_studies <- length(unique(results_df$study_id))
# Look at per study and test question, which prediction method performed the best?
error_metrics_per_test <- results_df |> filter(realization!=0 & abs(rel_error) < 10) |> group_by(study_id, test_question, prediction_method) |> 
  summarise(MAPE = mean(abs(rel_error)), MedAPE = median(abs(rel_error)), RMSPE = mean(rel_error^2), MAXPE = max(abs(rel_error)), .groups="drop")

# check the number of times each prediction method performed the best
best_prediction_method_per_study <- error_metrics_per_test |> 
  group_by(study_id, test_question) |> 
  summarise(MAPE = prediction_method[which.min(MAPE)], 
            MedAPE = prediction_method[which.min(MedAPE)],
            RMSPE= prediction_method[which.min(RMSPE)],
            MAXPE=prediction_method[which.min(MAXPE)]) 

prediction_types <- results_df$prediction_method |> unique() |> sort()
error_metrics <- colnames(best_prediction_method_per_study)[-c(1)]
prediction_types <- prediction_types |> purrr::lmap(\(type_l) {l <- list(); l[[type_l]]=NA_real_; l})
proportion_table <- tibble(error_metric = error_metrics, !!!prediction_types)

proportion_table_ <- best_prediction_method_per_study |> 
  pivot_longer(-c(study_id, test_question), names_to="error_metric", values_to = "prediction_method") |>
  group_by(error_metric, prediction_method) |> summarise(n=n()/nr_tests, .groups="drop")  |>
  pivot_wider(names_from = prediction_method, values_from = n) 
proportion_table <- dplyr::rows_update(proportion_table, proportion_table_)
# replace na with 0
proportion_table <- proportion_table |> mutate(across(where(is.numeric), ~replace_na(.x, 0)))

proportion_table_formatted <- proportion_table |> mutate(across(where(is.numeric), ~round(.x*100, digits=1))) |> mutate(across(where(is.numeric), ~paste(.x, "%"))) 
print(as.character(kable(proportion_table_formatted, "latex")))
proportion_table |> mutate(error_metric = paste0(error_metric, "_prop")) |> column_to_rownames("error_metric") |> t() |> data.frame() |> rownames_to_column(var="method")


```
# Look at rates of rejection
```{r}
results_df |> ggplot2::ggplot(aes(x=(experts-accepted_experts))) + geom_histogram(binwidth=1, center=0) + facet_wrap(facets=vars(prediction_method))
```


# Find and look at one of those high error estimates
```{r}

results_df |> filter(rel_error > 5000 & realization != 0) 

study_id_inspect <- 33
test_question_inspect <- 8

study_data <- studies_no_m_zero[[study_id_inspect]] 
test_set <- study_data |> filter(question_id==test_question_inspect) 
training_set <- study_data |> filter(question_id!=test_question_inspect) 

distributions <- interpolate_distributions(test_set |> add_0_and_100_percentiles(), interpolation="linear")
plot_distributions(distributions)

copulaDM <- create_copula_posterior_numerical_integration(indepCopula(10), distributions)
copula_fit_and_predict_JC_assumption(training_set, test_set, "indep", "linear")
equal_weight_predict(test_set)

```


```{r}
res_combined$results
```

