---
title: "copula_performance"
output: html_document
---

```{r setup}
library(tidyverse)
library(ggplot2)
library(purrr)
library(knitr)
library(patchwork)
library(copula)
library(scales)
library(knitr)
library(CopulaSEJ)
source("dev_utils.R")

file_name <- "output/data49_nov24.rds"
studies <- readRDS(file_name)
studies <- filter_studies_few_questions(studies, min_questions=10)
studies <- filter_study_remove_ids(studies, study_ids=7) 

percentile_names <- k_percentiles_to_colname(c(5, 50, 95))

list_mod <- change_value_in_study_list(studies, percentile_names, 0, 0.0001)
studies_no_m_zero <- list_mod[[1]]
modified_studies <- list_mod[[2]]

```

```{r}

#saveRDS(results_df, "dev/output/performance_comparison_new.rds")
#results_df<-readRDS("output/performance_comparison_new.rds")
results_df<-readRDS("output/compare_performance_simulation.rds")$results
results_df <- results_df |> filter(prediction_method == "CohierarchicalCDF3QNoRCT(0.7)")
benchmark_df<-readRDS("output/benchmarking_methods_performance.rds")$results
benchmark_df <- benchmark_df |> filter(study_id !=7) # remove study_id 7, not in the normal performance 

stopifnot(nrow(results_df)/length(unique(results_df$prediction_method)) == nrow(benchmark_df)/length(unique(benchmark_df$prediction_method)))

results_df <- rbind(results_df, benchmark_df)
nr_prediction_methods = length(unique(results_df$prediction_method))
#results_df <- readRDS("output/performance_comparison.rds")

#results_df <- results_df |> mutate(prediction_method = if_else(str_starts(prediction_method, "DP"), "product", prediction_method))

# results_df <- get_simulation_group_files(rel_dev=TRUE) |> load_files() |> combine_simulation_results()
# results_df <- results_df$results

pos_values <- results_df |> filter(realization>0 & prediction>0) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = "Postivie Predicted and Realized Values",
       x = "Realized Value",
       y = "Predicted Value") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal()

abs_values <- results_df |> filter(realization!=0 & prediction!=0) |> mutate(realization=abs(realization), prediction=abs(prediction)) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = "Predicted and Realized Values",
       x = "abs(Realized Value)",
       y = "abs(Predicted Value)") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal()

pos_values

abs_values
```
```{r, fig.width=10}
abs_values <- results_df |> filter(realization!=0 & prediction!=0) |> mutate(realization=abs(realization), prediction=abs(prediction)) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = "Predicted and Realized Values",
       x = "abs(Realized Value)",
       y = "abs(Predicted Value)") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal()
ggsave("output/abs_values.pdf", abs_values, width = 10, height = 10)
abs_values
```

# Look at relative error
```{r}
# count number of infinite rel_error
nr_0_realizations <- results_df |> filter(rel_error == Inf) |> nrow() / nr_prediction_methods
# should yield the same
nr_0_realizations_other <- results_df |> filter(realization == 0)  |> nrow() / nr_prediction_methods
stopifnot(nr_0_realizations == nr_0_realizations_other)
print(paste("Number of infinite relative errors: ", nr_0_realizations))


relative_erros_violins <- results_df |> filter(realization != 0 & abs(rel_error) < 10 & study_id <= 20) |> #filter(!(prediction_method %in% c("copula"))) |>
  ggplot(aes(prediction_method, rel_error)) +
  geom_violin() +
  labs(x = "method",
       y = "Relative Error") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("output/relative_erros_violins.png", relative_erros_violins, width=10)
relative_erros_violins 
```
```{r}
# some plot that per study shows the prediction of a specific method

max_rel_error_in_plt = 5

nr_studies <- max(results_df$study_id)
breaks <- unique(c(1, seq(from=10, to=nr_studies, by=10), nr_studies))
minor_b <- 1:nr_studies

rel_error_per_plot <- results_df |> filter(realization!=0 & abs(rel_error) < max_rel_error_in_plt) |> ggplot(aes(study_id, rel_error, group=study_id)) +
  geom_boxplot() + facet_wrap(facets=vars(prediction_method), ncol=1) + 
  scale_x_continuous(breaks = breaks, minor_breaks=minor_b, limits= c(1, nr_studies)) +
  labs(x = "Study ID",
       y = "Relative Error")

nr_filtered_points <- results_df |> filter(realization!=0 & abs(rel_error) < max_rel_error_in_plt) |> nrow()
nr_away_filtered_points <- results_df |> nrow() - nr_filtered_points
# lowest number of points for a study

ggsave("output/rel_error_per_plot_generalization.pdf", rel_error_per_plot, width = 8, height = 10)
rel_error_per_plot
```
```{r}
neg_log_lik_per_study <- ggplot(results_df |> filter(is.finite(neg_log_lik)), aes(study_id, neg_log_lik, group=study_id)) +
  geom_boxplot() + facet_grid(rows=vars(prediction_method)) + 
  scale_x_continuous(breaks = breaks, minor_breaks=minor_b) +
  labs(x = "Study ID",
       y = "Negative Log Likelihood") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
neg_log_lik_per_study
```

### Point results all samples
```{r}
options(pillar.print_min = 11)
results_df |> filter(realization!=0 & !is.na(rel_error)) |> group_by(prediction_method) |> summarise(MAPE = mean(abs(rel_error)),MedAPE = median(abs(rel_error)), RMSPE = sqrt(mean(rel_error^2)), MAXPE = max(abs(rel_error)), MedNLL=median(neg_log_lik)) 

```
### Point results rel_error less than 10
```{r}
results_df |> filter(realization!=0 & abs(rel_error) < 100 &  !is.na(rel_error)) |> group_by(prediction_method) |>  summarise(MAPE = mean(abs(rel_error)),MedAPE = median(abs(rel_error)), RMSPE = sqrt(mean(rel_error^2)), MAXPE = max(abs(rel_error)), MedNLL=median(neg_log_lik))
```
```{r}
results_df |> mutate(MAPE=abs(rel_error)) |> filter(realization!=0 & abs(rel_error) < 10 & !is.na(MAPE)) |> group_by(prediction_method) |>  summarise(MAPE = mean(MAPE),MedAPE = median(MAPE), RMSPE = sqrt(mean(MAPE^2)), MAXPE = max(MAPE), MedNLL=median(neg_log_lik))
```


```{r}
best_prediction_method_per_test <- results_df  |> group_by(study_id, test_question) |> 
  summarise(best_rel_error = min(abs(rel_error)), .groups="drop")

# join best_prediction_method_per_test and results_df
result_with_best <- results_df |> left_join(best_prediction_method_per_test, by=c("study_id", "test_question"))

times_equal_to_best_rel_error <- result_with_best |> rowwise() |>
  mutate(equal_to_best_rel_error = if_else(abs(rel_error) == best_rel_error, T, F)) |>
  group_by(prediction_method) |> summarise(n_equal_to_best_rel_error = sum(equal_to_best_rel_error), 
                                           ratio_best=n_equal_to_best_rel_error/nr_tests, .groups="drop") 
  
```


```{r}
nr_studies <- length(unique(results_df$study_id))
# Look at per study and test question, which prediction method performed the best?
error_metrics_per_test <- results_df |> filter(realization!=0 & abs(rel_error) < 10) |> group_by(study_id, test_question, prediction_method) |> 
  summarise(MAPE = mean(abs(rel_error)), MedAPE = median(abs(rel_error)), RMSPE = mean(rel_error^2), MAXPE = max(abs(rel_error)), .groups="drop")

# check the number of times each prediction method performed the best
best_prediction_method_per_study <- error_metrics_per_study |> 
  group_by(study_id, test_question) |> 
  summarise(MAPE = prediction_method[which.min(MAPE)], 
            MedAPE = prediction_method[which.min(MedAPE)],
            RMSPE= prediction_method[which.min(RMSPE)],
            MAXPE=prediction_method[which.min(MAXPE)]) 

prediction_types <- results_df$prediction_method |> unique() |> sort()
error_metrics <- colnames(best_prediction_method_per_study)[-c(1)]
prediction_types <- prediction_types |> purrr::lmap(\(type_l) {l <- list(); l[[type_l]]=NA_real_; l})
proportion_table <- tibble(error_metric = error_metrics, !!!prediction_types)

proportion_table_ <- best_prediction_method_per_study |> 
  pivot_longer(-c(study_id, test_question), names_to="error_metric", values_to = "prediction_method") |>
  group_by(error_metric, prediction_method) |> summarise(n=n()/nr_tests, .groups="drop")  |>
  pivot_wider(names_from = prediction_method, values_from = n) 
proportion_table <- dplyr::rows_update(proportion_table, proportion_table_)
# replace na with 0
proportion_table <- proportion_table |> mutate(across(where(is.numeric), ~replace_na(.x, 0)))

proportion_table_formatted <- proportion_table |> mutate(across(where(is.numeric), ~round(.x*100, digits=1))) |> mutate(across(where(is.numeric), ~paste(.x, "%"))) 
print(as.character(kable(proportion_table_formatted, "latex")))
proportion_table |> mutate(error_metric = paste0(error_metric, "_prop")) |> column_to_rownames("error_metric") |> t() |> data.frame() |> rownames_to_column(var="method")


```
# Look at rates of rejection
```{r}
results_df |> ggplot2::ggplot(aes(x=(experts-accepted_experts))) + geom_histogram(binwidth=1, center=0) + facet_wrap(facets=vars(prediction_method))
```


# Find and look at one of those high error estimates
```{r}

results_df |> filter(rel_error > 5000 & realization != 0) 

study_id_inspect <- 33
test_question_inspect <- 8

study_data <- studies_no_m_zero[[study_id_inspect]] 
test_set <- study_data |> filter(question_id==test_question_inspect) 
training_set <- study_data |> filter(question_id!=test_question_inspect) 

distributions <- interpolate_distributions(test_set |> add_0_and_100_percentiles(), interpolation="linear")
plot_distributions(distributions)

copulaDM <- create_copula_posterior_numerical_integration(indepCopula(10), distributions)
copula_fit_and_predict_JC_assumption(training_set, test_set, "indep", "linear")
equal_weight_predict(test_set)

```

