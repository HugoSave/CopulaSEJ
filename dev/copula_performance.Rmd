---
title: "copula_performance"
output: html_document
---

```{r setup}
library(tidyverse)
library(ggplot2)
library(purrr)
library(knitr)
library(patchwork)
library(copula)
library(scales)
library(knitr)
library(CopulaSEJ)
source("dev_utils.R")

file_name <- "output/data49_nov24.rds"
studies <- readRDS(file_name)
studies <- filter_studies_few_questions(studies, min_questions=10)
studies <- filter_study_remove_ids(studies, study_ids=7) 

percentile_names <- k_percentiles_to_colname(c(5, 50, 95))

list_mod <- change_value_in_study_list(studies, percentile_names, 0, 0.0001)
studies_no_m_zero <- list_mod[[1]]
modified_studies <- list_mod[[2]]

```

```{r}
data_list_short <- studies_no_m_zero[1:3]
params <- default_simulation_params(copula_model = "joe", prediction_method = "copula_calibration")
params <- default_simulation_params(copula_model = "frank", prediction_method = "copula")
params <- default_simulation_params(copula_model = "frank", prediction_method = "equal_weights")
params <- default_simulation_params(copula_model = "frank", prediction_method = "global_opt")
analysis_res <- run_analysis_per_study(data_list_short, params)

prediction_methods = c("copula_calibration", "copula_error_density", "equal_weights", "global_opt")
params <- default_simulation_params(copula_model = "joe", prediction_method = "perfect_expert")

# loop through studies in data_list_form
for (prediction_method in prediction_methods) {
  params <- default_simulation_params(copula_model = "joe", prediction_method = prediction_method)
  sim_res <- run_analysis_per_study(data_list_short, params)
  sim_res$results["prediction_method"] <- prediction_method
  results <- append(results, list(sim_res$results))
  warnings <- append(warnings, list(sim_res$warnings))
}
results_df <- dplyr::bind_rows(results)

#saveRDS(results_df, "dev/output/performance_comparison_new.rds")
#results_df<-readRDS("output/performance_comparison_new.rds")
results_df<-readRDS("output/compare_errors_and_summarizers_simulation.rds")$results_df
#results_df <- readRDS("output/performance_comparison.rds")

# results_df <- get_simulation_group_files(rel_dev=TRUE) |> load_files() |> combine_simulation_results()
# results_df <- results_df$results

pos_values <- results_df |> filter(realization>0 & prediction>0) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = "Postivie Predicted and Realized Values",
       x = "Realized Value",
       y = "Predicted Value") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal()

abs_values <- results_df |> filter(realization!=0 & prediction!=0) |> mutate(realization=abs(realization), prediction=abs(prediction)) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = "Predicted and Realized Values",
       x = "abs(Realized Value)",
       y = "abs(Predicted Value)") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal()

pos_values

abs_values
```
```{r, fig.width=10}
abs_values <- results_df |> filter(realization!=0 & prediction!=0) |> mutate(realization=abs(realization), prediction=abs(prediction)) |> 
  ggplot(aes(realization, prediction, color=prediction_method)) +
  geom_point() +
  # add dashed x=y line
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_fixed(ratio = 1) +
  labs(title = "Predicted and Realized Values",
       x = "abs(Realized Value)",
       y = "abs(Predicted Value)") +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans =  "log10")+
  facet_wrap(~prediction_method) +
  theme_minimal()
ggsave("Output/abs_values.pdf", abs_values, width = 10, height = 10)
abs_values
```

# Look at relative error
```{r}
# count number of infinite rel_error
nr_prediction_methods = length(unique(results_df$prediction_method))
nr_0_realizations <- results_df |> filter(rel_error == Inf) |> nrow() / nr_prediction_methods
# should yield the same
nr_0_realizations_other <- results_df |> filter(realization == 0)  |> nrow() / nr_prediction_methods
stopifnot(nr_0_realizations == nr_0_realizations_other)
print(paste("Number of infinite relative errors: ", nr_0_realizations))


relative_erros_violins <- results_df |> filter(realization != 0 & abs(rel_error) < 10 & study_id <= 20) |> #filter(!(prediction_method %in% c("copula"))) |>
  ggplot(aes(prediction_method, rel_error)) +
  geom_violin() +
  labs(x = "method",
       y = "Relative Error") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("output/relative_erros_violins.png", relative_erros_violins, width=10)
relative_erros_violins 
```
```{r}
# some plot that per study shows the prediction of a specific method

max_rel_error_in_plt = 10

nr_studies <- max(results_df$study_id)
breaks <- unique(c(1, seq(from=10, to=nr_studies, by=10), nr_studies))
minor_b <- 1:nr_studies

rel_error_per_plot <- results_df |> filter(realization!=0 & abs(rel_error) < max_rel_error_in_plt) |> ggplot(aes(study_id, rel_error, group=study_id)) +
  geom_boxplot() + facet_grid(rows=vars(prediction_method)) + 
  scale_x_continuous(breaks = breaks, minor_breaks=minor_b) +
  labs(x = "Study ID",
       y = "Relative Error")

nr_filtered_points <- results_df |> filter(realization!=0 & abs(rel_error) < max_rel_error_in_plt) |> nrow()
nr_away_filtered_points <- results_df |> nrow() - nr_filtered_points
# lowest number of points for a study

ggsave("output/rel_error_per_plot_generalization.pdf", rel_error_per_plot, width = 10, height = 10)
rel_error_per_plot
```
```{r}
neg_log_lik_per_study <- ggplot(results_df |> filter(is.finite(neg_log_lik)), aes(study_id, neg_log_lik, group=study_id)) +
  geom_boxplot() + facet_grid(rows=vars(prediction_method)) + 
  scale_x_continuous(breaks = breaks, minor_breaks=minor_b) +
  labs(x = "Study ID",
       y = "Negative Log Likelihood") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
neg_log_lik_per_study
```

### Point results all samples
```{r}

results_df |> filter(realization!=0 & !is.na(rel_error)) |> group_by(prediction_method) |> summarise(MAPE = mean(abs(rel_error)),MedAPE = median(abs(rel_error)), RMSPE = sqrt(mean(rel_error^2)), MAXPE = max(abs(rel_error)), MedNLL=median(neg_log_lik))

```
### Point results rel_error less than 10
```{r}
results_df |> filter(realization!=0 & abs(rel_error) < 10 &  !is.na(rel_error)) |> group_by(prediction_method) |>  summarise(MAPE = mean(abs(rel_error)),MedAPE = median(abs(rel_error)), RMSPE = sqrt(mean(rel_error^2)), MAXPE = max(abs(rel_error)), MedNLL=median(neg_log_lik))
```


```{r}
nr_studies <- length(unique(results_df$study_id))
# Look at per study, which prediction method performed the best?
error_metrics_per_study <- results_df |> filter(realization!=0 & abs(rel_error) < 10) |> group_by(study_id, prediction_method) |> 
  summarise(MAPE = mean(abs(rel_error)), RMSPE = mean(rel_error^2), MAXPE = max(abs(rel_error)), .groups="drop")

# check the number of times each prediction method performed the best
best_prediction_method_per_study <- error_metrics_per_study |> 
  group_by(study_id) |> 
  summarise(MAPE = prediction_method[which.min(MAPE)], 
            RMSPE= prediction_method[which.min(RMSPE)],
            MAXPE=prediction_method[which.min(MAXPE)]) 

proportion_table <- best_prediction_method_per_study |> 
  pivot_longer(-study_id, names_to="error_metric", values_to = "prediction_method") |>
  group_by(error_metric, prediction_method) |> summarise(n=n()/nr_studies, .groups="drop")  |>
  pivot_wider(names_from = prediction_method, values_from = n) 

proportion_table_formatted <- proportion_table |> mutate(across(where(is.numeric), ~round(.x*100, digits=1))) |> mutate(across(where(is.numeric), ~paste(.x, "%"))) 
print(as.character(kable(proportion_table_formatted, "latex")))
proportion_table

```

# Find and look at one of those high error estimates
```{r}

results_df |> filter(rel_error > 5000 & realization != 0) 

study_id_inspect <- 33
test_question_inspect <- 8

study_data <- studies_no_m_zero[[study_id_inspect]] 
test_set <- study_data |> filter(question_id==test_question_inspect) 
training_set <- study_data |> filter(question_id!=test_question_inspect) 

distributions <- interpolate_distributions(test_set |> add_0_and_100_percentiles(), interpolation="linear")
plot_distributions(distributions)

copulaDM <- create_copula_posterior_numerical_integration(indepCopula(10), distributions)
copula_fit_and_predict_JC_assumption(training_set, test_set, "indep", "linear")
equal_weight_predict(test_set)

```

